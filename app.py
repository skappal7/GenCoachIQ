import streamlit as st
import pandas as pd
import numpy as np
from pathlib import Path
import io
import json
from datetime import datetime, timedelta
import logging
from typing import Dict, List, Tuple, Optional
import asyncio
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# NLP and ML imports
import nltk
from textblob import TextBlob
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity

# Optional spaCy import with error handling
try:
    import spacy
    SPACY_AVAILABLE = True
except ImportError:
    SPACY_AVAILABLE = False
    spacy = None

# File processing imports
import openpyxl
import pyarrow as pa
import pyarrow.parquet as pq
from reportlab.lib.pagesizes import letter, A4
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.lib import colors

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Page configuration
st.set_page_config(
    page_title="GenCoachIQ",
    page_icon="üìû",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for professional styling
st.markdown("""
<style>
    .main-header {
        background: linear-gradient(90deg, #1e3c72 0%, #2a5298 100%);
        padding: 1rem;
        border-radius: 10px;
        color: white;
        text-align: center;
        margin-bottom: 2rem;
    }
    
    .metric-container {
        background: white;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        border-left: 4px solid #2a5298;
    }
    
    .config-section {
        background: #f8f9fa;
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1rem 0;
    }
    
    .success-box {
        background: #d4edda;
        border: 1px solid #c3e6cb;
        color: #155724;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    
    .warning-box {
        background: #fff3cd;
        border: 1px solid #ffeaa7;
        color: #856404;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    
    .error-box {
        background: #f8d7da;
        border: 1px solid #f5c6cb;
        color: #721c24;
        padding: 1rem;
        border-radius: 5px;
        margin: 1rem 0;
    }
    
    .stTabs [data-baseweb="tab-list"] {
        gap: 2px;
    }
    
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-left: 20px;
        padding-right: 20px;
        background-color: #f0f2f6;
        border-radius: 10px 10px 0px 0px;
    }
    
    .stTabs [aria-selected="true"] {
        background-color: #2a5298;
        color: white;
    }
</style>
""", unsafe_allow_html=True)

class CallAnalyticsConfig:
    """Configuration management for the application"""
    
    DEFAULT_SETTINGS = {
        "sentiment_threshold": 0.5,
        "nps_weights": {"positive": 0.4, "neutral": 0.3, "negative": 0.3},
        "compliance_keywords": [
            "terms and conditions", "privacy policy", "data protection",
            "opt-out", "consent", "agreement", "policy"
        ],
        "behavior_themes": [
            "empathy", "professionalism", "problem_solving", 
            "active_listening", "rapport_building", "solution_oriented"
        ],
        "opportunity_areas": [
            "response_time", "technical_knowledge", "communication_clarity",
            "follow_up", "issue_resolution", "customer_satisfaction"
        ]
    }
    
    @staticmethod
    def load_config() -> Dict:
        """Load configuration from session state or defaults"""
        if "app_config" not in st.session_state:
            st.session_state.app_config = CallAnalyticsConfig.DEFAULT_SETTINGS.copy()
        return st.session_state.app_config
    
    @staticmethod
    def save_config(config: Dict) -> None:
        """Save configuration to session state"""
        st.session_state.app_config = config

class DataProcessor:
    """Handles file processing and data conversion"""
    
    @staticmethod
    @st.cache_data(show_spinner=False)
    def excel_to_parquet(file_content: bytes, filename: str) -> Optional[pd.DataFrame]:
        """Convert Excel file to DataFrame efficiently"""
        try:
            # Read Excel file
            df = pd.read_excel(io.BytesIO(file_content), engine='openpyxl')
            
            # Basic data validation
            if df.empty:
                raise ValueError("Excel file is empty")
            
            # Ensure required columns exist (flexible column mapping)
            column_mapping = DataProcessor._detect_columns(df)
            if not column_mapping:
                raise ValueError("Could not detect transcript columns in the file")
            
            # Rename columns to standard format
            df = df.rename(columns=column_mapping)
            
            # Clean and preprocess data
            df = DataProcessor._clean_dataframe(df)
            
            logger.info(f"Successfully processed Excel file: {filename}")
            return df
            
        except Exception as e:
            logger.error(f"Error processing Excel file {filename}: {str(e)}")
            raise e
    
    @staticmethod
    def _detect_columns(df: pd.DataFrame) -> Dict[str, str]:
        """Detect and map relevant columns"""
        column_mapping = {}
        columns = df.columns.str.lower()
        
        # Common column patterns
        transcript_patterns = ['transcript', 'text', 'conversation', 'dialogue', 'content']
        agent_patterns = ['agent', 'representative', 'staff', 'employee']
        customer_patterns = ['customer', 'client', 'caller', 'user']
        date_patterns = ['date', 'time', 'timestamp', 'created']
        
        for col in df.columns:
            col_lower = col.lower()
            if any(pattern in col_lower for pattern in transcript_patterns):
                column_mapping[col] = 'transcript'
            elif any(pattern in col_lower for pattern in agent_patterns):
                column_mapping[col] = 'agent'
            elif any(pattern in col_lower for pattern in customer_patterns):
                column_mapping[col] = 'customer'
            elif any(pattern in col_lower for pattern in date_patterns):
                column_mapping[col] = 'date'
        
        return column_mapping
    
    @staticmethod
    def _clean_dataframe(df: pd.DataFrame) -> pd.DataFrame:
        """Clean and standardize DataFrame"""
        # Remove empty rows
        df = df.dropna(subset=['transcript'])
        
        # Clean transcript text
        if 'transcript' in df.columns:
            df['transcript'] = df['transcript'].astype(str).str.strip()
            df = df[df['transcript'] != '']
        
        # Add unique ID if not present
        if 'id' not in df.columns:
            df['id'] = range(1, len(df) + 1)
        
        # Convert date columns if present
        if 'date' in df.columns:
            try:
                df['date'] = pd.to_datetime(df['date'], errors='coerce')
            except:
                pass
        
        return df

class NLPAnalyzer:
    """Advanced NLP analysis for call transcripts"""
    
    def __init__(self):
        self._initialize_models()
    
    @st.cache_resource
    def _initialize_models(_self):
        """Initialize NLP models with caching"""
        try:
            # Load pre-trained sentiment analysis model
            _self.sentiment_analyzer = pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-roberta-base-sentiment-latest",
                return_all_scores=True
            )
            
            # Initialize spaCy model only if available
            _self.nlp = None
            if SPACY_AVAILABLE:
                try:
                    _self.nlp = spacy.load("en_core_web_sm")
                    st.success("‚úÖ spaCy model loaded successfully!")
                except OSError:
                    st.info("‚ÑπÔ∏è spaCy model not found. Using basic NLP features.")
                except Exception as spacy_error:
                    st.warning(f"‚ö†Ô∏è spaCy initialization failed: {str(spacy_error)}. Using basic NLP features.")
            else:
                st.info("‚ÑπÔ∏è spaCy not available. Using basic NLP features.")
            
            return True
        except Exception as e:
            logger.error(f"Error initializing NLP models: {str(e)}")
            st.error(f"‚ùå Model initialization failed: {str(e)}")
            return False
    
    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """Analyze sentiment with confidence scores"""
        try:
            if hasattr(self, 'sentiment_analyzer'):
                result = self.sentiment_analyzer(text[:512])  # Limit text length
                sentiment_scores = {item['label'].lower(): item['score'] for item in result[0]}
                return sentiment_scores
            else:
                # Fallback to TextBlob
                blob = TextBlob(text)
                polarity = blob.sentiment.polarity
                if polarity > 0.1:
                    return {'positive': 0.7, 'neutral': 0.2, 'negative': 0.1}
                elif polarity < -0.1:
                    return {'positive': 0.1, 'neutral': 0.2, 'negative': 0.7}
                else:
                    return {'positive': 0.3, 'neutral': 0.4, 'negative': 0.3}
        except Exception as e:
            logger.error(f"Sentiment analysis error: {str(e)}")
            return {'positive': 0.33, 'neutral': 0.34, 'negative': 0.33}
    
    def extract_themes(self, texts: List[str], n_themes: int = 5) -> List[str]:
        """Extract key themes using TF-IDF and clustering"""
        try:
            if len(texts) == 0:
                return []
            
            # TF-IDF Vectorization
            vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words='english',
                ngram_range=(1, 2),
                min_df=2
            )
            
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            # K-means clustering
            n_clusters = min(n_themes, len(texts))
            kmeans = KMeans(n_clusters=n_clusters, random_state=42)
            clusters = kmeans.fit_predict(tfidf_matrix)
            
            # Extract top terms for each cluster
            feature_names = vectorizer.get_feature_names_out()
            themes = []
            
            for i in range(n_clusters):
                cluster_center = kmeans.cluster_centers_[i]
                top_indices = cluster_center.argsort()[-5:][::-1]
                theme_words = [feature_names[idx] for idx in top_indices]
                themes.append(" ".join(theme_words))
            
            return themes
        except Exception as e:
            logger.error(f"Theme extraction error: {str(e)}")
            return ["general_discussion", "customer_service", "technical_support"]
    
    def calculate_nps_score(self, sentiment_scores: Dict[str, float], 
                          config: Dict) -> float:
        """Calculate NPS-like score based on sentiment"""
        try:
            weights = config.get("nps_weights", {"positive": 0.4, "neutral": 0.3, "negative": 0.3})
            
            nps_score = (
                sentiment_scores.get('positive', 0) * weights['positive'] * 100 +
                sentiment_scores.get('neutral', 0) * weights['neutral'] * 50 -
                sentiment_scores.get('negative', 0) * weights['negative'] * 25
            )
            
            return max(0, min(100, nps_score))
        except Exception as e:
            logger.error(f"NPS calculation error: {str(e)}")
            return 50.0
    
    def check_compliance(self, text: str, keywords: List[str]) -> Dict[str, any]:
        """Check compliance based on keywords"""
        try:
            text_lower = text.lower()
            found_keywords = [kw for kw in keywords if kw.lower() in text_lower]
            
            compliance_score = len(found_keywords) / len(keywords) * 100
            
            return {
                'score': compliance_score,
                'found_keywords': found_keywords,
                'missing_keywords': [kw for kw in keywords if kw not in found_keywords]
            }
        except Exception as e:
            logger.error(f"Compliance check error: {str(e)}")
            return {'score': 0, 'found_keywords': [], 'missing_keywords': keywords}

class ReportGenerator:
    """Generate reports in various formats"""
    
    @staticmethod
    def create_pdf_report(results_df: pd.DataFrame, summary_stats: Dict) -> bytes:
        """Generate professional PDF report"""
        try:
            buffer = io.BytesIO()
            doc = SimpleDocTemplate(buffer, pagesize=A4)
            styles = getSampleStyleSheet()
            story = []
            
            # Title
            title = Paragraph("Call Analytics Report", styles['Title'])
            story.append(title)
            story.append(Spacer(1, 12))
            
            # Summary Section
            summary_text = f"""
            <b>Analysis Summary</b><br/>
            Total Calls Analyzed: {summary_stats.get('total_calls', 0)}<br/>
            Average NPS Score: {summary_stats.get('avg_nps', 0):.1f}<br/>
            Average Sentiment Score: {summary_stats.get('avg_sentiment', 0):.1f}<br/>
            Compliance Rate: {summary_stats.get('compliance_rate', 0):.1f}%<br/>
            Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
            """
            
            summary_para = Paragraph(summary_text, styles['Normal'])
            story.append(summary_para)
            story.append(Spacer(1, 12))
            
            # Top Issues Table
            if not results_df.empty:
                story.append(Paragraph("<b>Top Opportunities for Improvement</b>", styles['Heading2']))
                
                # Create table data
                table_data = [['Call ID', 'NPS Score', 'Sentiment', 'Key Themes']]
                for _, row in results_df.head(10).iterrows():
                    table_data.append([
                        str(row.get('id', '')),
                        f"{row.get('nps_score', 0):.1f}",
                        row.get('primary_sentiment', ''),
                        str(row.get('themes', ''))[:50] + "..."
                    ])
                
                table = Table(table_data)
                table.setStyle(TableStyle([
                    ('BACKGROUND', (0, 0), (-1, 0), colors.grey),
                    ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
                    ('ALIGN', (0, 0), (-1, -1), 'CENTER'),
                    ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                    ('FONTSIZE', (0, 0), (-1, 0), 10),
                    ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                    ('BACKGROUND', (0, 1), (-1, -1), colors.beige),
                    ('GRID', (0, 0), (-1, -1), 1, colors.black)
                ]))
                
                story.append(table)
            
            doc.build(story)
            buffer.seek(0)
            return buffer.getvalue()
            
        except Exception as e:
            logger.error(f"PDF generation error: {str(e)}")
            raise e

class CallAnalyticsApp:
    """Main application class"""
    
    def __init__(self):
        self.config = CallAnalyticsConfig.load_config()
        self.nlp_analyzer = NLPAnalyzer()
        self.processor = DataProcessor()
        self.report_generator = ReportGenerator()
    
    def run(self):
        """Main application entry point"""
        # Header
        st.markdown("""
        <div class="main-header">
            <h1>üìû Call Analytics Pro</h1>
            <p>Advanced ML-powered Call Transcript Analysis & Coaching Insights</p>
        </div>
        """, unsafe_allow_html=True)
        
        # Main tabs
        tabs = st.tabs([
            "üè† Dashboard", 
            "üì§ Upload & Process", 
            "‚öôÔ∏è Configuration", 
            "üìä Results & Analytics",
            "üìñ User Guide"
        ])
        
        with tabs[0]:
            self._render_dashboard()
        
        with tabs[1]:
            self._render_upload_process()
        
        with tabs[2]:
            self._render_configuration()
            
        with tabs[3]:
            self._render_results()
        
        with tabs[4]:
            self._render_user_guide()
    
    def _render_dashboard(self):
        """Render main dashboard"""
        st.header("üìà Analytics Overview")
        
        # Check if we have processed data
        if "analysis_results" not in st.session_state:
            st.info("üëã Welcome! Upload your call transcript files to get started with the analysis.")
            
            # Quick stats placeholders
            col1, col2, col3, col4 = st.columns(4)
            
            with col1:
                st.markdown("""
                <div class="metric-container">
                    <h3>0</h3>
                    <p>Calls Processed</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col2:
                st.markdown("""
                <div class="metric-container">
                    <h3>-</h3>
                    <p>Avg NPS Score</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col3:
                st.markdown("""
                <div class="metric-container">
                    <h3>-</h3>
                    <p>Compliance Rate</p>
                </div>
                """, unsafe_allow_html=True)
            
            with col4:
                st.markdown("""
                <div class="metric-container">
                    <h3>-</h3>
                    <p>Sentiment Score</p>
                </div>
                """, unsafe_allow_html=True)
            
            return
        
        # Display actual metrics
        results_df = st.session_state.analysis_results
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            total_calls = len(results_df)
            st.metric("Total Calls", total_calls)
        
        with col2:
            avg_nps = results_df['nps_score'].mean() if 'nps_score' in results_df.columns else 0
            st.metric("Avg NPS Score", f"{avg_nps:.1f}")
        
        with col3:
            avg_compliance = results_df['compliance_score'].mean() if 'compliance_score' in results_df.columns else 0
            st.metric("Compliance Rate", f"{avg_compliance:.1f}%")
        
        with col4:
            avg_sentiment = results_df['sentiment_positive'].mean() if 'sentiment_positive' in results_df.columns else 0
            st.metric("Avg Sentiment", f"{avg_sentiment:.2f}")
        
        # Charts
        col1, col2 = st.columns(2)
        
        with col1:
            if 'primary_sentiment' in results_df.columns:
                fig = px.pie(
                    results_df, 
                    names='primary_sentiment', 
                    title="Sentiment Distribution",
                    color_discrete_map={'positive': '#2E8B57', 'negative': '#DC143C', 'neutral': '#FFD700'}
                )
                st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            if 'nps_score' in results_df.columns:
                fig = px.histogram(
                    results_df, 
                    x='nps_score', 
                    nbins=20, 
                    title="NPS Score Distribution",
                    color_discrete_sequence=['#2a5298']
                )
                st.plotly_chart(fig, use_container_width=True)
    
    def _render_upload_process(self):
        """Render upload and processing interface"""
        st.header("üì§ Upload & Process Call Transcripts")
        
        col1, col2 = st.columns([2, 1])
        
        with col1:
            # File upload
            uploaded_file = st.file_uploader(
                "Choose Excel file containing call transcripts",
                type=['xlsx', 'xls'],
                help="Upload Excel files up to 500MB. Ensure your file contains transcript data."
            )
            
            if uploaded_file:
                file_details = {
                    "filename": uploaded_file.name,
                    "filetype": uploaded_file.type,
                    "filesize": f"{uploaded_file.size / (1024*1024):.1f} MB"
                }
                
                st.success(f"‚úÖ File uploaded: {file_details['filename']} ({file_details['filesize']})")
        
        with col2:
            st.markdown("""
            <div class="config-section">
                <h4>üìã Processing Options</h4>
                <p>‚Ä¢ Automatic column detection</p>
                <p>‚Ä¢ Sentiment analysis</p>
                <p>‚Ä¢ NPS prediction</p>
                <p>‚Ä¢ Compliance checking</p>
                <p>‚Ä¢ Theme extraction</p>
            </div>
            """, unsafe_allow_html=True)
        
        # Process button
        if uploaded_file:
            if st.button("üöÄ Start Analysis", type="primary", use_container_width=True):
                self._process_file(uploaded_file)
    
    def _process_file(self, uploaded_file):
        """Process uploaded file with progress tracking"""
        try:
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # Step 1: File conversion
            status_text.text("üìÑ Converting Excel to Parquet format...")
            progress_bar.progress(10)
            
            df = self.processor.excel_to_parquet(
                uploaded_file.getvalue(), 
                uploaded_file.name
            )
            
            if df is None or df.empty:
                st.error("‚ùå Failed to process file. Please check file format.")
                return
            
            progress_bar.progress(25)
            
            # Step 2: Initialize analysis
            status_text.text("üß† Initializing NLP models...")
            progress_bar.progress(40)
            
            # Step 3: Analyze transcripts
            status_text.text("üìä Analyzing transcripts...")
            results = []
            
            total_rows = len(df)
            for idx, row in df.iterrows():
                transcript = str(row.get('transcript', ''))
                
                if len(transcript.strip()) == 0:
                    continue
                
                # Sentiment analysis
                sentiment_scores = self.nlp_analyzer.analyze_sentiment(transcript)
                primary_sentiment = max(sentiment_scores, key=sentiment_scores.get)
                
                # NPS calculation
                nps_score = self.nlp_analyzer.calculate_nps_score(sentiment_scores, self.config)
                
                # Compliance check
                compliance_result = self.nlp_analyzer.check_compliance(
                    transcript, 
                    self.config['compliance_keywords']
                )
                
                # Store results
                result = {
                    'id': row.get('id', idx + 1),
                    'transcript': transcript[:200] + "..." if len(transcript) > 200 else transcript,
                    'agent': row.get('agent', 'Unknown'),
                    'customer': row.get('customer', 'Unknown'),
                    'date': row.get('date', datetime.now()),
                    'sentiment_positive': sentiment_scores.get('positive', 0),
                    'sentiment_neutral': sentiment_scores.get('neutral', 0),
                    'sentiment_negative': sentiment_scores.get('negative', 0),
                    'primary_sentiment': primary_sentiment,
                    'nps_score': nps_score,
                    'compliance_score': compliance_result['score'],
                    'compliance_keywords_found': ', '.join(compliance_result['found_keywords']),
                    'compliance_keywords_missing': ', '.join(compliance_result['missing_keywords']),
                }
                
                results.append(result)
                
                # Update progress
                progress = 40 + int((idx / total_rows) * 50)
                progress_bar.progress(progress)
            
            # Step 4: Extract themes
            status_text.text("üîç Extracting coaching themes...")
            transcripts = [r['transcript'] for r in results]
            themes = self.nlp_analyzer.extract_themes(transcripts)
            
            # Add themes to results
            for result in results:
                result['themes'] = ', '.join(themes[:3])  # Top 3 themes
            
            progress_bar.progress(90)
            
            # Step 5: Save results
            status_text.text("üíæ Saving analysis results...")
            results_df = pd.DataFrame(results)
            st.session_state.analysis_results = results_df
            st.session_state.analysis_summary = {
                'total_calls': len(results_df),
                'avg_nps': results_df['nps_score'].mean(),
                'avg_sentiment': results_df['sentiment_positive'].mean(),
                'compliance_rate': results_df['compliance_score'].mean(),
                'processing_date': datetime.now(),
                'themes': themes
            }
            
            progress_bar.progress(100)
            status_text.text("‚úÖ Analysis completed successfully!")
            
            st.success(f"üéâ Successfully processed {len(results_df)} call transcripts!")
            
            # Show quick preview
            with st.expander("üìã Quick Preview", expanded=True):
                st.dataframe(results_df.head(10), use_container_width=True)
            
        except Exception as e:
            st.error(f"‚ùå Processing failed: {str(e)}")
            logger.error(f"File processing error: {str(e)}")
    
    def _render_configuration(self):
        """Render configuration interface"""
        st.header("‚öôÔ∏è Analysis Configuration")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("üéØ Sentiment Analysis Settings")
            
            sentiment_threshold = st.slider(
                "Sentiment Confidence Threshold",
                min_value=0.1,
                max_value=1.0,
                value=self.config.get('sentiment_threshold', 0.5),
                step=0.1,
                help="Minimum confidence required for sentiment classification"
            )
            
            st.subheader("üìä NPS Calculation Weights")
            
            col_pos, col_neu, col_neg = st.columns(3)
            with col_pos:
                pos_weight = st.number_input(
                    "Positive Weight",
                    min_value=0.0,
                    max_value=1.0,
                    value=self.config.get('nps_weights', {}).get('positive', 0.4),
                    step=0.1
                )
            
            with col_neu:
                neu_weight = st.number_input(
                    "Neutral Weight",
                    min_value=0.0,
                    max_value=1.0,
                    value=self.config.get('nps_weights', {}).get('neutral', 0.3),
                    step=0.1
                )
            
            with col_neg:
                neg_weight = st.number_input(
                    "Negative Weight",
                    min_value=0.0,
                    max_value=1.0,
                    value=self.config.get('nps_weights', {}).get('negative', 0.3),
                    step=0.1
                )
        
        with col2:
            st.subheader("üîç Compliance Keywords")
            
            compliance_keywords = st.text_area(
                "Compliance Keywords (one per line)",
                value='\n'.join(self.config.get('compliance_keywords', [])),
                height=150,
                help="Keywords to check for regulatory compliance"
            )
            
            st.subheader("üéØ Coaching Themes")
            
            behavior_themes = st.text_area(
                "Behavior Themes (one per line)",
                value='\n'.join(self.config.get('behavior_themes', [])),
                height=100,
                help="Key behaviors to identify and score"
            )
            
            opportunity_areas = st.text_area(
                "Opportunity Areas (one per line)",
                value='\n'.join(self.config.get('opportunity_areas', [])),
                height=100,
                help="Areas for potential improvement"
            )
        
        # Save configuration
        col1, col2, col3 = st.columns([1, 1, 1])
        
        with col2:
            if st.button("üíæ Save Configuration", type="primary", use_container_width=True):
                new_config = {
                    'sentiment_threshold': sentiment_threshold,
                    'nps_weights': {
                        'positive': pos_weight,
                        'neutral': neu_weight,
                        'negative': neg_weight
                    },
                    'compliance_keywords': [kw.strip() for kw in compliance_keywords.split('\n') if kw.strip()],
                    'behavior_themes': [theme.strip() for theme in behavior_themes.split('\n') if theme.strip()],
                    'opportunity_areas': [area.strip() for area in opportunity_areas.split('\n') if area.strip()]
                }
                
                CallAnalyticsConfig.save_config(new_config)
                self.config = new_config
                
                st.success("‚úÖ Configuration saved successfully!")
        
        # Reset to defaults
        with col3:
            if st.button("üîÑ Reset to Defaults", use_container_width=True):
                CallAnalyticsConfig.save_config(CallAnalyticsConfig.DEFAULT_SETTINGS.copy())
                self.config = CallAnalyticsConfig.DEFAULT_SETTINGS.copy()
                st.success("‚úÖ Configuration reset to defaults!")
                st.rerun()
    
    def _render_results(self):
        """Render results and analytics interface"""
        st.header("üìä Results & Analytics")
        
        if "analysis_results" not in st.session_state:
            st.info("üîÑ No analysis results available. Please upload and process data first.")
            return
        
        results_df = st.session_state.analysis_results
        summary_stats = st.session_state.analysis_summary
        
        # Summary Statistics
        st.subheader("üìà Analysis Summary")
        
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            st.metric("Total Calls", summary_stats['total_calls'])
        
        with col2:
            st.metric("Avg NPS Score", f"{summary_stats['avg_nps']:.1f}")
        
        with col3:
            st.metric("Avg Sentiment", f"{summary_stats['avg_sentiment']:.2f}")
        
        with col4:
            st.metric("Compliance Rate", f"{summary_stats['compliance_rate']:.1f}%")
        
        with col5:
            processed_date = summary_stats['processing_date'].strftime("%Y-%m-%d %H:%M")
            st.metric("Processed", processed_date)
        
        # Detailed Analytics
        st.subheader("üìä Detailed Analytics")
        
        tab1, tab2, tab3, tab4 = st.tabs(["üìã Data Table", "üìà Visualizations", "üéØ Insights", "üì§ Export"])
        
        with tab1:
            # Filters
            col1, col2, col3 = st.columns(3)
            
            with col1:
                sentiment_filter = st.selectbox(
                    "Filter by Sentiment",
                    ["All", "positive", "negative", "neutral"]
                )
            
            with col2:
                nps_range = st.slider(
                    "NPS Score Range",
                    min_value=0.0,
                    max_value=100.0,
                    value=(0.0, 100.0)
                )
            
            with col3:
                compliance_threshold = st.number_input(
                    "Min Compliance Score",
                    min_value=0.0,
                    max_value=100.0,
                    value=0.0
                )
            
            # Apply filters
            filtered_df = results_df.copy()
            
            if sentiment_filter != "All":
                filtered_df = filtered_df[filtered_df['primary_sentiment'] == sentiment_filter]
            
            filtered_df = filtered_df[
                (filtered_df['nps_score'] >= nps_range[0]) &
                (filtered_df['nps_score'] <= nps_range[1]) &
                (filtered_df['compliance_score'] >= compliance_threshold)
            ]
            
            st.dataframe(
                filtered_df,
                use_container_width=True,
                hide_index=True,
                column_config={
                    "nps_score": st.column_config.ProgressColumn(
                        "NPS Score",
                        help="Net Promoter Score prediction",
                        min_value=0,
                        max_value=100,
                    ),
                    "compliance_score": st.column_config.ProgressColumn(
                        "Compliance Score",
                        help="Compliance adherence percentage",
                        min_value=0,
                        max_value=100,
                    ),
                    "sentiment_positive": st.column_config.NumberColumn(
                        "Positive Sentiment",
                        format="%.2f"
                    )
                }
            )
        
        with tab2:
            # Visualizations
            col1, col2 = st.columns(2)
            
            with col1:
                # Sentiment vs NPS scatter plot
                fig = px.scatter(
                    results_df,
                    x='sentiment_positive',
                    y='nps_score',
                    color='primary_sentiment',
                    size='compliance_score',
                    title="Sentiment vs NPS Score",
                    labels={
                        'sentiment_positive': 'Positive Sentiment Score',
                        'nps_score': 'NPS Score'
                    }
                )
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                # Compliance distribution
                fig = px.box(
                    results_df,
                    x='primary_sentiment',
                    y='compliance_score',
                    title="Compliance Score by Sentiment",
                    color='primary_sentiment'
                )
                st.plotly_chart(fig, use_container_width=True)
            
            # Time series analysis (if date available)
            if 'date' in results_df.columns:
                results_df['date'] = pd.to_datetime(results_df['date'], errors='coerce')
                daily_stats = results_df.groupby(results_df['date'].dt.date).agg({
                    'nps_score': 'mean',
                    'sentiment_positive': 'mean',
                    'compliance_score': 'mean'
                }).reset_index()
                
                fig = make_subplots(
                    rows=3, cols=1,
                    subplot_titles=("Daily Average NPS Score", "Daily Average Sentiment", "Daily Compliance Rate"),
                    vertical_spacing=0.1
                )
                
                fig.add_trace(
                    go.Scatter(x=daily_stats['date'], y=daily_stats['nps_score'], name="NPS"),
                    row=1, col=1
                )
                
                fig.add_trace(
                    go.Scatter(x=daily_stats['date'], y=daily_stats['sentiment_positive'], name="Sentiment"),
                    row=2, col=1
                )
                
                fig.add_trace(
                    go.Scatter(x=daily_stats['date'], y=daily_stats['compliance_score'], name="Compliance"),
                    row=3, col=1
                )
                
                fig.update_layout(height=600, title_text="Time Series Analysis")
                st.plotly_chart(fig, use_container_width=True)
        
        with tab3:
            # Key Insights
            st.subheader("üîç Key Insights")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("#### üéØ Top Performing Calls")
                top_calls = results_df.nlargest(5, 'nps_score')[['id', 'nps_score', 'primary_sentiment', 'compliance_score']]
                st.dataframe(top_calls, hide_index=True)
                
                st.markdown("#### üìä Sentiment Distribution")
                sentiment_counts = results_df['primary_sentiment'].value_counts()
                for sentiment, count in sentiment_counts.items():
                    percentage = (count / len(results_df)) * 100
                    st.write(f"**{sentiment.title()}**: {count} calls ({percentage:.1f}%)")
            
            with col2:
                st.markdown("#### ‚ö†Ô∏è Opportunities for Improvement")
                low_performing = results_df.nsmallest(5, 'nps_score')[['id', 'nps_score', 'primary_sentiment', 'compliance_score']]
                st.dataframe(low_performing, hide_index=True)
                
                st.markdown("#### üéØ Common Themes")
                if 'themes' in summary_stats:
                    for i, theme in enumerate(summary_stats['themes'][:5], 1):
                        st.write(f"**{i}.** {theme}")
            
            # Recommendations
            st.markdown("#### üí° Recommendations")
            
            avg_nps = summary_stats['avg_nps']
            avg_compliance = summary_stats['compliance_rate']
            
            recommendations = []
            
            if avg_nps < 60:
                recommendations.append("üî¥ **Critical**: NPS score is below average. Focus on customer satisfaction training.")
            elif avg_nps < 75:
                recommendations.append("üü° **Improvement Needed**: NPS score has room for improvement. Consider advanced coaching.")
            else:
                recommendations.append("üü¢ **Good Performance**: NPS scores are healthy. Maintain current practices.")
            
            if avg_compliance < 70:
                recommendations.append("üî¥ **Compliance Risk**: Low compliance scores detected. Review training materials.")
            elif avg_compliance < 85:
                recommendations.append("üü° **Compliance Watch**: Compliance scores need attention. Implement regular audits.")
            else:
                recommendations.append("üü¢ **Compliant**: Good compliance adherence. Continue monitoring.")
            
            negative_sentiment_pct = (results_df['primary_sentiment'] == 'negative').mean() * 100
            if negative_sentiment_pct > 30:
                recommendations.append("üî¥ **Sentiment Alert**: High negative sentiment detected. Investigate root causes.")
            
            for rec in recommendations:
                st.markdown(rec)
        
        with tab4:
            # Export options
            st.subheader("üì§ Export Results")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                # CSV Export
                csv_data = results_df.to_csv(index=False)
                st.download_button(
                    label="üìÑ Download CSV",
                    data=csv_data,
                    file_name=f"call_analytics_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                    mime="text/csv",
                    use_container_width=True
                )
            
            with col2:
                # Excel Export
                excel_buffer = io.BytesIO()
                with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                    results_df.to_excel(writer, sheet_name='Analysis Results', index=False)
                    
                    # Add summary sheet
                    summary_df = pd.DataFrame([summary_stats])
                    summary_df.to_excel(writer, sheet_name='Summary', index=False)
                
                excel_data = excel_buffer.getvalue()
                
                st.download_button(
                    label="üìä Download Excel",
                    data=excel_data,
                    file_name=f"call_analytics_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                    use_container_width=True
                )
            
            with col3:
                # PDF Report Export
                try:
                    pdf_data = self.report_generator.create_pdf_report(results_df, summary_stats)
                    st.download_button(
                        label="üìë Download PDF Report",
                        data=pdf_data,
                        file_name=f"call_analytics_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf",
                        mime="application/pdf",
                        use_container_width=True
                    )
                except Exception as e:
                    st.error(f"PDF generation failed: {str(e)}")
            
            # Power BI preparation
            st.markdown("---")
            st.subheader("üîó Power BI Integration")
            
            st.info("""
            **For Power BI Integration:**
            1. Download the CSV file above
            2. Import into Power BI using 'Get Data' ‚Üí 'Text/CSV'
            3. Use the following key columns for visualizations:
               - `nps_score` for NPS dashboards
               - `primary_sentiment` for sentiment analysis
               - `compliance_score` for compliance tracking
               - `themes` for coaching insights
            """)
    
    def _render_user_guide(self):
        """Render comprehensive user guide"""
        st.header("üìñ User Guide")
        
        guide_tabs = st.tabs([
            "üöÄ Getting Started", 
            "üìä Understanding Results", 
            "‚öôÔ∏è Configuration Guide",
            "üîß Troubleshooting",
            "‚ùì FAQ"
        ])
        
        with guide_tabs[0]:
            st.markdown("""
            ## üöÄ Getting Started with Call Analytics Pro
            
            ### Step 1: Prepare Your Data
            - **File Format**: Excel files (.xlsx, .xls) up to 500MB
            - **Required Columns**: The app will automatically detect columns containing:
              - Call transcripts/conversation text
              - Agent/representative information
              - Customer/caller information
              - Date/timestamp (optional)
            
            ### Step 2: Upload and Process
            1. Go to the **Upload & Process** tab
            2. Click "Choose Excel file" and select your transcript file
            3. Click "üöÄ Start Analysis" to begin processing
            4. Monitor the progress bar and status updates
            
            ### Step 3: Review Results
            - Navigate to **Results & Analytics** to view detailed analysis
            - Use filters to focus on specific segments
            - Export results in CSV, Excel, or PDF format
            
            ### Step 4: Configure Settings (Optional)
            - Adjust sentiment analysis parameters in **Configuration**
            - Customize compliance keywords for your industry
            - Set up coaching themes specific to your organization
            """)
        
        with guide_tabs[1]:
            st.markdown("""
            ## üìä Understanding Your Results
            
            ### Key Metrics Explained
            
            #### NPS Score (0-100)
            - **80-100**: Excellent customer experience
            - **60-79**: Good performance with room for improvement
            - **40-59**: Average performance, coaching recommended
            - **0-39**: Poor performance, immediate attention needed
            
            #### Sentiment Scores
            - **Positive**: Customer satisfaction indicators
            - **Neutral**: Balanced or transactional interactions
            - **Negative**: Dissatisfaction or frustration signals
            
            #### Compliance Score (0-100%)
            - Percentage of required keywords/phrases found
            - Based on regulatory and company policy requirements
            - Higher scores indicate better adherence to guidelines
            
            ### Coaching Insights
            
            #### Themes
            - Automatically extracted topics from conversations
            - Help identify common discussion points
            - Useful for training focus areas
            
            #### Opportunities
            - Areas where agents can improve performance
            - Based on sentiment, compliance, and NPS analysis
            - Prioritized by impact and frequency
            """)
        
        with guide_tabs[2]:
            st.markdown("""
            ## ‚öôÔ∏è Configuration Guide
            
            ### Sentiment Analysis Settings
            
            #### Confidence Threshold (0.1 - 1.0)
            - Higher values = more conservative sentiment classification
            - Lower values = more sensitive to emotional indicators
            - **Recommended**: 0.5 for balanced results
            
            ### NPS Calculation Weights
            
            Configure how sentiment scores contribute to NPS prediction:
            - **Positive Weight**: Impact of positive sentiment
            - **Neutral Weight**: Impact of neutral sentiment  
            - **Negative Weight**: Impact of negative sentiment
            
            **Best Practice**: Ensure weights sum close to 1.0
            
            ### Compliance Keywords
            
            Add industry-specific terms that agents should mention:
            - Regulatory requirements (GDPR, CCPA, etc.)
            - Company policies
            - Legal disclaimers
            - Process confirmations
            
            ### Coaching Themes
            
            Define behavioral indicators to track:
            - Empathy expressions
            - Active listening cues
            - Problem-solving approaches
            - Professional language
            
            ### Opportunity Areas
            
            Specify improvement categories:
            - Technical knowledge gaps
            - Communication skills
            - Process adherence
            - Customer service excellence
            """)
        
        with guide_tabs[3]:
            st.markdown("""
            ## üîß Troubleshooting
            
            ### Common Issues and Solutions
            
            #### "Failed to process file" Error
            **Possible Causes:**
            - File is corrupted or password protected
            - Unsupported Excel format
            - No transcript data detected
            
            **Solutions:**
            - Re-save file as .xlsx format
            - Remove password protection
            - Ensure transcript columns contain text data
            - Check file size (max 500MB)
            
            #### "No analysis results available" Message
            **Cause:** Processing hasn't been completed or failed
            
            **Solution:**
            - Return to Upload & Process tab
            - Re-upload and process your file
            - Check browser console for error messages
            
            #### Slow Processing Performance
            **Causes:**
            - Large file size
            - Complex transcript content
            - Limited system resources
            
            **Solutions:**
            - Split large files into smaller batches
            - Close other browser tabs
            - Wait for processing to complete
            
            #### PDF Export Not Working
            **Cause:** Browser or system limitations
            
            **Solution:**
            - Use CSV or Excel export instead
            - Try in a different browser
            - Reduce result set size with filters
            
            ### Performance Optimization Tips
            
            1. **File Preparation**
               - Remove unnecessary columns before upload
               - Ensure consistent data formatting
               - Split files larger than 300MB
            
            2. **Browser Settings**
               - Use Chrome or Firefox for best performance
               - Ensure adequate RAM available
               - Clear browser cache if experiencing issues
            
            3. **Processing Strategy**
               - Process during off-peak hours for better performance
               - Start with smaller sample files to test configuration
               - Save results immediately after processing
            """)
        
        with guide_tabs[4]:
            st.markdown("""
            ## ‚ùì Frequently Asked Questions
            
            ### General Questions
            
            **Q: What file formats are supported?**
            A: Excel files (.xlsx and .xls) up to 500MB in size.
            
            **Q: How long does processing take?**
            A: Processing time varies by file size. Typical times:
            - Small files (1-50MB): 2-5 minutes
            - Medium files (50-200MB): 5-15 minutes  
            - Large files (200-500MB): 15-30 minutes
            
            **Q: Is my data secure?**
            A: Yes, all processing happens in your browser session. No data is permanently stored on servers.
            
            **Q: Can I process multiple files at once?**
            A: Currently, the app processes one file at a time. You can combine multiple files into a single Excel workbook.
            
            ### Technical Questions
            
            **Q: Why are some NPS scores showing as 0?**
            A: This may occur with very short transcripts or those lacking emotional indicators. Check your NPS weight configuration.
            
            **Q: How accurate is the sentiment analysis?**
            A: The app uses advanced transformer models with 85-90% accuracy on customer service conversations. Accuracy may vary with specialized terminology.
            
            **Q: Can I customize the compliance keywords?**
            A: Yes, go to Configuration tab to add industry-specific terms and requirements.
            
            **Q: What happens if I refresh the browser?**
            A: Processed results are stored in session cache, but you'll need to re-upload files if you refresh before processing completes.
            
            ### Integration Questions
            
            **Q: How do I use results in Power BI?**
            A: Export as CSV, then import into Power BI using "Get Data" ‚Üí "Text/CSV". The file structure is optimized for Power BI dashboards.
            
            **Q: Can I automate this process?**
            A: The current version requires manual upload. For automation, consider the API version of similar tools.
            
            **Q: What's the maximum number of calls I can analyze?**
            A: Limited by file size (500MB) rather than call count. Typically 10,000-50,000 calls depending on transcript length.
            
            ### Support
            
            **Need Additional Help?**
            - Check the troubleshooting section above
            - Review your file format and structure  
            - Try with a smaller sample file first
            - Ensure your browser supports modern JavaScript features
            """)

# Initialize and run the application
if __name__ == "__main__":
    try:
        # Download required NLTK data
        import ssl
        try:
            _create_unverified_https_context = ssl._create_unverified_context
        except AttributeError:
            pass
        else:
            ssl._create_default_https_context = _create_unverified_https_context
        
        nltk.download('punkt', quiet=True)
        nltk.download('stopwords', quiet=True)
        
        # Initialize and run app
        app = CallAnalyticsApp()
        app.run()
        
    except Exception as e:
        st.error(f"Application initialization failed: {str(e)}")
        st.info("Please refresh the page and try again.")
        logger.error(f"App initialization error: {str(e)}")
